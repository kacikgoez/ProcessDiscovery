{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#orca-organ-retrieval-and-information-collection-analytics","title":"ORCA: Organ Retrieval and (Information) Collection Analytics","text":""},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>ORCA: Organ Retrieval and (Information) Collection Analytics</li> <li>Table of Contents</li> <li>Introduction: Analyzing and Visualizing the Organ Donation Process<ul> <li>Background</li> <li>The Dataset</li> <li>Extraction of the Event Log</li> </ul> </li> <li>Installation<ul> <li>Prerequisites</li> <li>Clone the repository</li> <li>Acquire the raw dataset</li> <li>Build and run the dashboard</li> </ul> </li> <li>User Interface<ul> <li>Overview</li> <li>Visual Tour</li> <li>Default Layout</li> <li>Header</li> <li>Help</li> <li>Global Filters</li> <li>Download the Event Log</li> <li>Add a Tile</li> <li>Grid</li> <li>Configure a Tile</li> </ul> </li> <li>Features and Functions<ul> <li>Filtering</li> <li>Available Visualizations</li> <li>Variants</li> <li>Distribution</li> <li>KPIs</li> <li>DFG</li> <li>De-Jure Process</li> </ul> </li> <li>Troubleshooting</li> <li>Glossary</li> <li>References</li> </ul>"},{"location":"#introduction-analyzing-and-visualizing-the-organ-donation-process","title":"Introduction: Analyzing and Visualizing the Organ Donation Process","text":"<p>This is a web-based dashboard for an M.Sc. student project at RWTH university. It is a dashboard written in Vue.js and Flask that shows KPIs for Organ Donation Process.  The dataset \"Organ Retrieval and Collection of Health Information for Donation\" (ORCHID, Adam. H et al.) is supplied by PhysioNet and accessible on https://physionet.org/content/orchid/1.0.0/.</p>"},{"location":"#background","title":"Background","text":"<p>Organ donation is a life-saving procedure that involves the removal of organs from a deceased or living donor and their transplantation into a recipient. For patients with end-stage organ failure, organ transplantation is often the only treatment option. However, the demand for organs far exceeds the supply. Prior research has focused on designing better allocation policies to distribute organs to patients on the waiting list. Yet the organ shortage remains a significant problem, with over 100,000 patients on the waiting list in the United States alone. To address this problem, researchers have recently started to examine the process by which organs are procured from deceased donors. Currently, the organ donation process is complex and involves many stakeholders, including hospitals, organ procurement organizations (OPOs), and transplant centers. The process is also highly variable, with significant differences in the number of organs procured across OPOs. In addition, there are well-documented inefficiencies in the process, such as the high rate of consent refusal by next-of-kin (NOK). This leads to a large number of missed opportunities for procuring transplant-viable organs. Procurement deficiencies have a disproportionate impact on Black, Indigenous, or people of color (BIPOC) individuals due to the higher prevalence of diseases requiring transplantation within these communities.  In addition, certain Organ Procurement Organizations (OPOs) face specific challenges in obtaining organs from BIPOC donors, which contributes to inequities since these organs are more likely to be medically compatible for BIPOC patients awaiting transplants.  The improvement of organ procurement is a significant public health concern. Each year, approximately 28,000 organs go untransplanted, resulting in the tragic loss of over 10,000 patients who are waiting for suitable transplants.</p> <p>Understanding the organ donation process is a crucial first step towards improving the process. However, until now, there has been no publicly available dataset that provides a comprehensive view of the organ donation process.</p>"},{"location":"#the-dataset","title":"The Dataset","text":"<p>The ORCHID dataset contains information about the organ donation process. It covers ten years of clinical, financial, and administrative information from six organ procurement organizations (OPOs) in the United States. The dataset is structured based on the standard procedure of organ procurement at an OPO.  The procurement process consists of six main stages: referral, evaluation, approach, authorization, procurement, and transplant:</p> <ol> <li>Referral: When a hospitalized patient is in critical condition, the hospital refers them to the local OPO for potential organ donation.</li> <li>Evaluation: The OPO assesses each referral, conducting an initial evaluation of the patient's suitability for organ donation.</li> <li>Approach: If the referred patient is found medically suitable, a representative from the OPO approaches the patient's next-of-kin (NOK) to seek their consent for donation.</li> <li>Authorization: Upon obtaining consent, the OPO oversees the procurement of transplant-ready organs from the deceased patient.</li> <li>Procurement: The OPO presents each obtained organ to individuals on the national transplant waitlist, prioritizing recipients based on the ranking established by the Organ Procurement and Transplantation Network (OPTN). The organ is assigned to the highest-ranked patient whose transplant surgeon accepts the offer.</li> <li>Transplant: Finally, the OPO manages the logistics of transporting the organ to the recipient's transplant center, facilitating the transplant procedure.</li> </ol> <p>To adhere to the Health Insurance Portability and Accountability Act (HIPAA) standards, all data underwent de-identification through structured data cleansing and date shifting.  This involved eliminating all eighteen identifiable data elements specified in HIPAA, such as patient name, address, and dates.  For patients aged 89 and above, their precise age was concealed, and they are represented in the dataset with an age of 100.  Dates were systematically shifted into the future with a random offset for each patient.  It is important to note that the date-shifting process maintained intervals, ensuring, for example, the preservation of the time span between death and the Organ Procurement Organization (OPO) approach for each patient.</p> <p>In total, the dataset covers 133,101 deceased donor referrals and 8,972 organ donations across 13 states.  For each patient, the dataset contains information about the patient and the process data (flags that indicate if the patient was approached, authorized, etc. and timestamps for these events). Listed below are the attributes that are available for each patient. For more information, please see the data description:</p> Column Description <code>PatientID</code> A unique identifier for each patient. We use this as the case id. <code>OPO</code> The OPO that is responsible for the patient. <code>HospitalID</code> The hospital where the patient was treated. <code>Age</code> The age of the patient. <code>Gender</code> The gender of the patient. <code>Race</code> The race of the patient. <code>brain_death</code> Indicates whether the patient experienced brain death. <code>Referral_Year</code> The year of patient referral. <code>Referral_DayofWeek</code> The day of the week of patient referral. <code>Cause_of_Death_UNOS</code> The cause of death according to UNOS (United Network for Organ Sharing). <code>Mechanism_of_Death</code> UNOS defined mechanism of death. <code>Circumstances_of_Death</code> UNOS defined circumstances of death. <code>outcome_heart</code> Outcome for the heart organ. <code>outcome_liver</code> Outcome for the liver organ. <code>outcome_kidney_left</code> Outcome for the left kidney organ. <code>outcome_kidney_right</code> Outcome for the right kidney organ. <code>outcome_lung_left</code> Outcome for the left lung organ. <code>outcome_lung_right</code> Outcome for the right lung organ. <code>outcome_pancreas</code> Outcome for the pancreas organ."},{"location":"#extraction-of-the-event-log","title":"Extraction of the Event Log","text":"<p>The raw dataset contains one row for each patient. The row contains information about the patient and process data (flags that indicate if the patient was approached, authorized, etc. and timestamps for these events). To analyze the process, we need to extract an event log from the raw dataset. The event log contains one row for each event. The row contains information about the patient and the event (e.g., timestamp, event type). We use the patient id as the case id and the event type as the activity name. We rename the columns to match the standard format of an event log.</p> Column Description <code>case:concept:name</code> A unique identifier for each case. We use the patient id. <code>event:timestamp</code> The timestamp of the event. <code>concept:name</code> The name of the event (activity). We use the six main stages of the organ donation process. <p>In addition, we also rename the other columns to be a bit more consistent. Please see the table here for more information.</p> <p>However, the raw dataset contains no timestamps for the events \"Evaluation\" and \"Transplant\".  Therefore, we assume that these events happen one minute after the previous event. This is a simplification, but it is sufficient for our purposes, and ensures that the events are in the correct order. If timestamps for the other events are missing, we filter out the corresponding cases. This is because we cannot determine the correct order of the events if timestamps are missing. This attributes to roughly 3000 cases that are filtered out.</p> <p>The event log will be automatically extracted when you run the dashboard for the first time. After that, the event log will be stored in the folder <code>backend/data/processed/</code>. Note, when running the dashboard with Docker, the event log will NOT be stored on your host machine. It will only be stored in the Docker container. In this case, you can download the event log on the dashboard. The event log will be downloaded as a CSV file.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>You can easily run the dashboard on your local machine using Docker. Make sure you have Docker installed on your machine.  If you don't have Docker installed, you can download it here: https://www.docker.com/products/docker-desktop. In addition, you need to have a reasonably modern browser installed. The dashboard was tested with the latest versions of Chrome, Firefox and Safari.</p>"},{"location":"#clone-the-repository","title":"Clone the repository","text":"<p>The repository is publically available, to clone it you can run the following <code>git</code> command:</p> <pre><code>git clone https://github.com/kacikgoez/ProcessDiscovery.git\n</code></pre>"},{"location":"#acquire-the-raw-dataset","title":"Acquire the raw dataset","text":"<p>The raw dataset is not included in this repository as it is a restricted dataset. You can download it from https://physionet.org/content/orchid/1.0.0/. For that, you need to create an account on PhysioNet and agree to the data use agreement. After that, you can download the dataset as a zip file. Unzip the file and place the content in the folder <code>backend/data/raw/</code>. The folder structure should look like this: <pre><code>backend/data/raw/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 calc_deaths.csv\n\u251c\u2500\u2500 data_description.html\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 opd.csv\n\u2514\u2500\u2500 SHA256SUMS.txt\n</code></pre> Note that for the dashboard to work, you only need the file <code>opd.csv</code>. The other files are not used. Please do not rename the file.</p> <p>The raw dataset will automatically be processed when you run the dashboard for the first time. This may take a few minutes.</p>"},{"location":"#build-and-run-the-dashboard","title":"Build and run the dashboard","text":"<p>To build and run the dashboard, open a terminal and navigate to the root folder of the project. Then run the following command: <pre><code>docker-compose up --build\n</code></pre> This will build the docker images and run the dashboard. You can access the dashboard on http://localhost:80.</p>"},{"location":"#user-interface","title":"User Interface","text":""},{"location":"#overview","title":"Overview","text":"<p>The dashboard is a single-page application. It can be subdivided into two parts: the header and the grid.  We will explain each part in more detail below.</p>"},{"location":"#visual-tour","title":"Visual Tour","text":"<p>When first opening the dashboard, you will be greeted with a visual tour. The tour will guide you through the dashboard and explain the different features. You can either dismiss the tour completely (will not be shown again), skip the tour, or go through the tour step by step. The tour can be restarted at any time by clicking on the button (1) in the top left corner of the dashboard (see here for more information). We would recommend going through the tour at least once to get a better understanding of the dashboard.</p>"},{"location":"#default-layout","title":"Default Layout","text":"<p>The dashboard has a default layout that consists of four tiles. The tiles show the following visualizations: variants, distribution, KPIs, and DFG. TODO: Add image of default layout</p> <p></p>"},{"location":"#header","title":"Header","text":"<p>In the header, you will find the following elements:</p>"},{"location":"#help","title":"Help","text":"<p>Click the question mark icon (1) to open the help page.  On the help page, you can either restart the visual tour or reset the dashboard to the default layout.</p>"},{"location":"#global-filters","title":"Global Filters","text":"<p>In the middle of the header, you will find the global filters. These filters are applied to all visualizations. Please see here for more information how to use the filters.</p>"},{"location":"#download-the-event-log","title":"Download the Event Log","text":"<p>You can download the event log as a CSV file by clicking on the button (2) in the top right corner of the dashboard. The event log will be downloaded as a CSV file. The event log contains a row for each event. The row contains information about the patient and the event (e.g., timestamp, event type). See here for more information. You can use the event log to analyze the process using other tools such as ProM or PM4Py.</p>"},{"location":"#add-a-tile","title":"Add a Tile","text":"<p>You can add a new tile by clicking on the button (3) in the top right corner of the dashboard. </p>"},{"location":"#grid","title":"Grid","text":"<p>The grid is the main part of the dashboard. It contains the tiles that show the visualizations. You can add a new tile by clicking on the button in the top right corner of the dashboard. The tiles can be moved by dragging them. You can also resize the tiles by dragging the button (2) in the bottom right corner of the tile. Lastly, you can delete a tile by clicking on the button (3) in the top right corner of the tile.</p> <p></p> <p>In the header of each tile, you will find the following elements:</p> <ul> <li>The title of the tile</li> <li>The download button (1) to download the visualization as a PNG file</li> <li>The edit button (2) to configure the tile</li> <li>The delete button (3) to delete the tile</li> </ul> <p>In the body of each tile, you will find the visualization. Please see here for more information about the available visualizations and how to use them. All visualizations are interactive. You can hover over the visualization to see more information. You can also click on the visualization to filter the other visualizations.</p> <p>In the footer of each tile, you will find the filters of the tile. These filters are only applied to the corresponding visualization.  Please see here for more information how to use the filters.</p> <p>The state of the dashboard is saved in the browser's local storage. This means that the dashboard will remember the layout and the configuration of the tiles when you close the browser. However, if you clear the browser's local storage, the dashboard will be reset to the default layout.  You can also reset the dashboard to the default layout by clicking on the button (1) in the top left corner of the dashboard (see here for more information).</p>"},{"location":"#configure-a-tile","title":"Configure a Tile","text":"<p>You can configure a tile by clicking on the edit button (2) in the top right corner of the tile. Here you can change the visualization to be shown in the tile and the disaggregation attribute(s) to be used for that visualization. The visualization can be selected by clicking the according list item. For the KPIs visualization, you can select multiple KPIs. Please remember to scroll down to see all available visualizations. All selected visualizations will be marked with a blue background. The disaggregation attribute(s) can be selected with the dropdown menu (2). Lastly, you can change the title of the tile. Just click on the text (1) and type a new title.</p> <p>After completing the configuration, you need to click on the button (3) to save the changes.</p>"},{"location":"#features-and-functions","title":"Features and Functions","text":""},{"location":"#filtering","title":"Filtering","text":"<p>The dashboard supports two types of filters: global filters and individual filters. The global filters are applied to all visualizations. The individual filters are only applied to the corresponding visualization (in addition to the global filters). Multiple filters are combined with the logical operator \"AND\".</p> <p>You can add a global filter by clicking on the \"Add filter\" button in the top center of the dashboard.  Individual filters can be added by clicking on the \"Add filter\" button in the footer of the tile.</p> <p>Each filter consists of an attribute, an operator, and a value.  For the attribute, you can choose from the attributes that are available for each patient (see here for more information) or from process attributes. Note that we treat all patient attributes except the patient age as categorical attributes. The following table shows the available process attributes:</p> Attribute Attribute Type Description Start activity Categorical The first activity of the case. End activity Categorical The last activity of the case. Variant Categorical The variant of the case (sequence of activities). Case duration Numerical The duration of the case from first to last event (in seconds). Case size Numerical The number of events in the case. <p></p> <p>The attributes are grouped (either patient or process attributes).  You can also search for an attribute by typing in the search bar and then selecting the attribute from the dropdown menu. By default, a \"IS NOT EMPTY\" filter is added. This filter is used to filter out cases that do not have a value for the selected attribute. You can change the filter by clicking on the filter itself (1). This will open a dropdown menu where you can select the operator and the value (if necessary).</p> <p>For the operator, you can choose from the following operators (depending on the attribute type):</p> Operator Attribute Type Value Description IS EMPTY Both None The attribute is empty. IS NOT EMPTY Both None The attribute is not empty. EQUALS Both Single The attribute equals the value. NOT EQUALS Both Single The attribute does not equal the value. CONTAINS Categorical Multiple The attribute contains the value. NOT CONTAINS Categorical Multiple The attribute does not contain the value. LESS THAN Numerical Single The attribute is less than the value. LESS THAN OR EQUALS Numerical Single The attribute is less than or equal to the value. GREATER THAN Numerical Single The attribute is greater than the value. GREATER THAN OR EQUALS Numerical Single The attribute is greater than or equal to the value. <p></p> <p>Depending on the operator, you must enter none, one, or multiple values. For example, if you choose the operator \"CONTAINS\", you must enter multiple values. For categorical attributes, you can select the value(s) via a dropdown menu. For numerical attributes, you must enter the value manually.</p>"},{"location":"#available-visualizations","title":"Available Visualizations","text":""},{"location":"#variants","title":"Variants","text":"<p>You can get an overview of the variants of the process by using the \"Variants\" visualization. This visualization lists all variants of the process. A variant is a sequence of events that occurred in the process.</p> <p></p> <p>Each variant is represented by a chevron diagram. The chevron diagram shows the sequence of events for each variant. The activity names are abbreviated to fit the diagram. You can hover over the diagram to see the full activity name. On the right side of the diagram, you can see a pie chart that shows the number of cases for each variant based on the selected disaggregation attribute. If you hover over a pie chart, you can see the number of cases for each variant for the corresponding disaggregation attribute value. In addition, in the middle of the pie chart the overall percentage how often the variant occurs is shown. Note that the variants are sorted in descending order based on the overall percentage how often the variant occurs.</p>"},{"location":"#distribution","title":"Distribution","text":"<p>You can select a disaggregation attribute and then select a pie chart to view the distribution of the attribute across all cases.</p> <p></p>"},{"location":"#kpis","title":"KPIs","text":"<p>You can assess process conformance and performance through the following six KPIs visualizations.  The first three KPIs focus on deviations from the process path, and the last three KPIs focus on the duration of the process.  By clicking multiple KPIs, you can also add them to a tile. </p> <p></p> <p>Happy Path Adherence measures the proportion of patients who follow the predefined, optimal process flow (de jure process) for their care pathway. The de jure process is the standard process that is defined by the OPO (Referral -&gt; Evaluation -&gt; Approach -&gt; Authorization -&gt; Procurement -&gt; Transplant). The metric is calculated as the ratio of the number of patients following the de jure process to the total number of patients in the group. The group can be selected using one or two disaggregation attributes. Identifying deviations from the happy path can help healthcare providers to pinpoint process inefficiencies, understand the reasons for non-adherence, and develop interventions to improve compliance with the care pathway.</p> <p>Dropout Rate calculates the rate at which patients discontinue or drop out from their prescribed care pathway at each stage of the process. The metric is calculated by analyzing the event log to determine the last recorded stage for each patient. The number of dropouts at each stage is then aggregated based on the selected disaggregation attribute. By understanding where and why patients are dropping out, healthcare organizations can tailor interventions to address specific challenges, thereby improving patient retention and outcomes.</p> <p>Permuted Path Adherence assesses the extent to which patient care pathways differ from the standard (de jure) process. The de jure process is the standard process that is defined by the OPO (Referral -&gt; Evaluation -&gt; Approach -&gt; Authorization -&gt; Procurement -&gt; Transplant). The metric is calculated by identifying all patient pathways that do not strictly follow the de jure process. These pathways are then counted and categorized based on the selected disaggregation attribute(s). Analyzing permuted paths can reveal innovative practices or necessary adaptations to the standard care process. It can also help in identifying best practices and areas for standardization.</p> <p>Bureaucratic Duration measures the time taken from referral to procurement, highlighting the efficiency of the administrative and logistical aspects of the care pathway. The metric is calculated by calculating the total duration from the referral event to the procurement event for each patient or case. The average duration is then calculated based on the selected disaggregation attribute(s). Shortening bureaucratic duration can lead to faster treatment initiation, improved patient experience, and reduced costs. This metric helps in pinpointing delays and inefficiencies in the process.</p> <p>Evaluation to Approach measures the time interval between the evaluation and approach stages in the patient care pathway. The metric is calculated by calculating the total duration between the evaluation and approach stages for each patient or case. The average duration is then calculated based on the selected disaggregation attribute(s). Reducing the time between evaluation and approach can accelerate patient access to care, potentially improving outcomes by enabling timely treatment.</p> <p>Authorization to Procurement quantifies the duration between obtaining treatment authorization and the procurement of necessary services or treatments. The metric is calculated by measuring the average time from authorization to procurement across different patient groups or treatment categories. The average duration is then calculated based on the selected disaggregation attribute(s). Streamlining the authorization to procurement process can reduce wait times for patients, improve resource utilization, and enhance overall process efficiency.</p>"},{"location":"#dfg","title":"DFG","text":"<p>You can select this visualization to see the paths for all cases, including happy paths and permuted paths.  The nodes in the graph represent activities, and the edges give the number of the directly following relation. You can hover over the edge to see the full source-to-target relationship with number.  The edges in red have the highest number in the dfg.</p> <p></p>"},{"location":"#de-jure-process","title":"De-Jure Process","text":"<p>For the dejure process, you can select the visualization using different performance statistics for a disaggregation attribute.  The nodes in the graph represent activities and the edges give the statistics.</p> Statistics Description MAX The max duration between two activities MIN The min duration between two activities MEDIAN The median duration between two activities MEAN The mean duration between two activities REMAIN The percentage of activity that goes to the next activity DROP The percentage of disaggregated patient that drop in each activity"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Ensure that port 80 is free when using Docker. Use <code>netstat</code> or <code>lsof</code> to check whether the ports are free.</p> </li> <li> <p>Use the reset button to reset the layout if it is bugged, which can sometimes happen for unknown reasons.</p> </li> </ul>"},{"location":"#glossary","title":"Glossary","text":"<p>Approach: The OPO approaches the patient. This happens when a representative from the OPO approaches the patient's next-of-kin (NOK) to seek their consent for donation. In the dataset, a flag is available that indicates if the patient was approached. If the patient was approached, a timestamp is available.</p> <p>Authorization: The patient is authorized for organ donation. This happens when the OPO obtains consent from the NOK for donation. In the dataset, a flag is available that indicates if the patient was authorized. If the patient was authorized, a timestamp is available.</p> <p>BIPOC: Black, Indigenous, or people of color. A term used to describe people who are not white.</p> <p>De-jure process: In this setting the de-jure process refers to the standard order of the organ donation process that is defined by the OPO (Referral -&gt; Evaluation -&gt; Approach -&gt; Authorization -&gt; Procurement -&gt; Transplant).</p> <p>Evaluation: The patient is evaluated by the OPO. This happens when the OPO assesses each referral, conducting an initial evaluation of the patient's suitability for organ donation. This event is always present for each patient. However, no timestamp is available. Therefore, we assume that the event happens one minute after the previous event (Referral).</p> <p>Event: An event is an action that happens during the organ donation process. The events are Referral, Evaluation, Approach, Authorization, Procurement, and Transplant. The events should happen in the above order. However, the dataset contains some inconsistencies. For example, there are cases where the patient was authorized before the patient was approached. Each event has a timestamp and a type (e.g., Referral, Evaluation, etc.) and corresponds to a case (patient) in the event log.</p> <p>HIPAA: Health Insurance Portability and Accountability Act. A law that protects the privacy of patients' health information.</p> <p>NOK: Next-of-kin. The next-of-kin is the closest living relative of a patient. The next-of-kin is responsible for making medical decisions for the patient if they are unable to do so.</p> <p>OPO: Organ Procurement Organization. An organization that is responsible for the organ donation process.</p> <p>ORCA: The name of the dashboard. It stands for Organ Retrieval and (Information) Collection Analytics.</p> <p>ORCHID: The name of the raw dataset. It stands for Organ Retrieval and Collection of Health Information for Donation. It is supplied by PhysioNet and accessible on https://physionet.org/content/orchid/1.0.0/.</p> <p>Procurement: The organs are procured. This happens when the OPO oversees the procurement of transplant-ready organs from the deceased patient. In the dataset, a flag is available that indicates if the organs were procured. If the organs were procured, a timestamp is available.</p> <p>Referral: The patient is referred to the OPO. This happens when the patient is in critical condition and the hospital refers them to the local OPO for potential organ donation. This event is always present for each patient and a timestamp is available.</p> <p>Transplant: The organs are transplanted. This happens when the OPO manages the logistics of transporting the organ to the recipient's transplant center, facilitating the transplant procedure. In the dataset, a flag is available that indicates if the organs were transplanted. However, no timestamp is available. Therefore, we assume that the event happens one minute after the previous event (Procurement).</p> <p>UNOS: United Network for Organ Sharing. A non-profit organization that manages the nation's organ transplant system under contract with the federal government. UNOS maintains the national transplant waiting list, matching donors to recipients 24 hours a day, 365 days a year. </p>"},{"location":"#references","title":"References","text":"<ol> <li>Adam, H., Suriyakumar, V., Pollard, T., Moody, B., Erickson, J., Segal, G., Adams, B., Brockmeier, D., Lee, K., McBride, G., Ranum, K., Wadsworth, M., Whaley, J., Wilson, A., &amp; Ghassemi, M. (2023). Organ Retrieval and Collection of Health Information for Donation (ORCHID) (version 1.0.0). PhysioNet. https://doi.org/10.13026/eytj-4f29.</li> <li>Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... &amp; Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215\u2013e220.</li> </ol>"},{"location":"dataclasses/","title":"Dataclasses","text":""},{"location":"dataclasses/#dataclasses-used-in-the-project","title":"Dataclasses used in the project","text":""},{"location":"dataclasses/#backend.src.dataclasses.attributes","title":"<code>attributes</code>","text":""},{"location":"dataclasses/#backend.src.dataclasses.attributes.PatientAttribute","title":"<code>PatientAttribute: UnionType = CategoricalAttribute | NumericalAttribute</code>  <code>module-attribute</code>","text":"<p>A patient attribute. Can be either a  CategoricalAttribute or a  NumericalAttribute.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.AttributeType","title":"<code>AttributeType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enumeration of the attribute types.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.AttributeType.CATEGORICAL","title":"<code>CATEGORICAL = 'categorical'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The categorical attribute type.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.AttributeType.NUMERICAL","title":"<code>NUMERICAL = 'numerical'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The numerical attribute type.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.CategoricalAttribute","title":"<code>CategoricalAttribute</code>  <code>dataclass</code>","text":"<p>A categorical attribute.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the attribute.</p> <code>values</code> <code>list[str]</code> <p>The values of the attribute.</p> <code>type</code> <code>str</code> <p>The type of the attribute. Defaults to AttributeType.CATEGORICAL.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.DisaggregationAttribute","title":"<code>DisaggregationAttribute</code>  <code>dataclass</code>","text":"<p>A disaggregation attribute. This will be used to disaggregate the data into different groups. For numerical attributes,the bins will be used to create the groups. For categorical attributes, the values will be used to create the groups.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the attribute.</p> <code>type</code> <code>AttributeType</code> <p>The type of the attribute.</p> <code>bins</code> <code>list[int]</code> <p>The bins of the numerical attribute. Defaults to None.</p>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.DisaggregationAttribute.get_bin_labels","title":"<code>get_bin_labels(include_infinities=False)</code>","text":"<p>Returns the bin labels of the numerical attribute.</p> <p>Example: <pre><code># create a numerical attribute with bins\nattribute = DisaggregationAttribute('attribute', AttributeType.NUMERICAL, bins=[0, 5, 10])\n\n# get the bin labels\nbin_labels = attribute.get_bin_labels(include_infinities=True)\nprint(bin_labels)\n# ['&lt; 0', '0 - 5', '5 - 10', '&gt; 10']\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>include_infinities</code> <code>bool</code> <p>Whether to include the infinities in the bin labels. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>The bin labels.</p> Source code in <code>backend/src/dataclasses/attributes.py</code> <pre><code>def get_bin_labels(self, include_infinities: bool = False) -&gt; list[str]:\n    \"\"\"Returns the bin labels of the numerical attribute.\n\n    Example:\n    ```python\n    # create a numerical attribute with bins\n    attribute = DisaggregationAttribute('attribute', AttributeType.NUMERICAL, bins=[0, 5, 10])\n\n    # get the bin labels\n    bin_labels = attribute.get_bin_labels(include_infinities=True)\n    print(bin_labels)\n    # ['&lt; 0', '0 - 5', '5 - 10', '&gt; 10']\n    ```\n\n    Args:\n        include_infinities (bool, optional): Whether to include the infinities in the bin labels. Defaults to False.\n\n    Returns:\n        (list[str]): The bin labels.\n    \"\"\"\n    if self.type == AttributeType.NUMERICAL:\n        if include_infinities:\n            return [f'&lt; {self.bins[0]}'] + [f'{self.bins[i]} - {self.bins[i + 1]}' for i in\n                                            range(len(self.bins) - 1)] + [f'&gt; {self.bins[-1]}']\n        else:\n            return [f'{self.bins[i]} - {self.bins[i + 1]}' for i in range(len(self.bins) - 1)]\n    else:\n        raise ValueError('The bin labels are only available for numerical attributes.')\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.DisaggregationAttribute.get_bins","title":"<code>get_bins(include_infinities=False)</code>","text":"<p>Returns the bins of the numerical attribute.</p> <p>Example: <pre><code># create a numerical attribute with bins\nattribute = DisaggregationAttribute('attribute', AttributeType.NUMERICAL, bins=[0, 5, 10])\n\n# get the bins\nbins = attribute.get_bins(include_infinities=True)\nprint(bins)\n# [-inf, 0, 5, 10, inf]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>include_infinities</code> <code>bool</code> <p>Whether to include the infinities in the bins. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>The bins.</p> Source code in <code>backend/src/dataclasses/attributes.py</code> <pre><code>def get_bins(self, include_infinities: bool = False) -&gt; list[float]:\n    \"\"\"Returns the bins of the numerical attribute.\n\n    Example:\n    ```python\n    # create a numerical attribute with bins\n    attribute = DisaggregationAttribute('attribute', AttributeType.NUMERICAL, bins=[0, 5, 10])\n\n    # get the bins\n    bins = attribute.get_bins(include_infinities=True)\n    print(bins)\n    # [-inf, 0, 5, 10, inf]\n    ```\n\n    Args:\n        include_infinities (bool, optional): Whether to include the infinities in the bins. Defaults to False.\n\n    Returns:\n        (list[float]): The bins.\n    \"\"\"\n    if self.type == AttributeType.NUMERICAL:\n        if include_infinities:\n            return [-float('inf')] + self.bins + [float('inf')]\n        else:\n            return self.bins\n    else:\n        raise ValueError('The bins are only available for numerical attributes.')\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.attributes.NumericalAttribute","title":"<code>NumericalAttribute</code>  <code>dataclass</code>","text":"<p>A numerical attribute.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the attribute.</p> <code>min</code> <code>float</code> <p>The minimum value of the attribute.</p> <code>max</code> <code>float</code> <p>The maximum value of the attribute.</p> <code>type</code> <code>str</code> <p>The type of the attribute. Defaults to AttributeType.NUMERICAL.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts","title":"<code>charts</code>","text":""},{"location":"dataclasses/#backend.src.dataclasses.charts.DataItem","title":"<code>DataItem</code>  <code>dataclass</code>","text":"<p>A data item.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>str | int | float</code> <p>The x value of the data item.</p> <code>y</code> <code>float</code> <p>The y value of the data item.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.DataSeries","title":"<code>DataSeries</code>  <code>dataclass</code>","text":"<p>A data series.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the data series.</p> <code>data</code> <code>list[DataItem]</code> <p>The data of the data series.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.DataSeries.from_dict","title":"<code>from_dict(name, data, sort_by=None)</code>  <code>classmethod</code>","text":"<p>Create a data series from a dictionary. The keys of the dictionary will be used as x values and the values of the dictionary will be used as y values. The data will be sorted by the x values. If the sort_by parameter is given and a sort order is defined for the given key, the data will be sorted by the sort order.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the data series.</p> required <code>data</code> <code>dict[str | int | float, float]</code> <p>The data of the data series.</p> required <code>sort_by</code> <code>str | None</code> <p>The key to sort the data by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataSeries</code> <code>DataSeries</code> <p>The data series.</p> Source code in <code>backend/src/dataclasses/charts.py</code> <pre><code>@classmethod\ndef from_dict(cls, name: str, data: dict[str | int | float, float], sort_by: str | None = None) -&gt; \"DataSeries\":\n    \"\"\"\n    Create a data series from a dictionary. The keys of the dictionary will be used as x values and the values of\n    the dictionary will be used as y values. The data will be sorted by the x values. If the sort_by parameter is\n    given and a sort order is defined for the given key, the data will be sorted by the sort order.\n\n    Args:\n        name (str): The name of the data series.\n        data (dict[str | int | float, float]): The data of the data series.\n        sort_by (str | None): The key to sort the data by.\n\n    Returns:\n        DataSeries: The data series.\n    \"\"\"\n\n    # sort the data by the x values\n    if sort_by in SORT_ORDERS:\n        data = dict(sorted(data.items(), key=lambda item: SORT_ORDERS[sort_by].index(item[0])))\n    else:\n        data = dict(sorted(data.items(), key=lambda item: item[0]))\n\n    return DataSeries(\n        name=name,\n        data=[DataItem(x=x, y=0 if pd.isna(y) else y) for x, y in data.items()])\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.charts.Edge","title":"<code>Edge</code>  <code>dataclass</code>","text":"<p>An edge in a graph.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source node.</p> <code>target</code> <code>str</code> <p>The target node.</p> <code>label</code> <code>str | None</code> <p>The label of the edge.</p> <code>value</code> <code>float | None</code> <p>The value of the edge.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.Graph","title":"<code>Graph</code>  <code>dataclass</code>","text":"<p>A graph.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the graph.</p> <code>nodes</code> <code>list[Node]</code> <p>The nodes of the graph.</p> <code>edges</code> <code>list[Edge]</code> <p>The edges of the graph.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.Graph.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check that the nodes have unique ids and that the edges are valid.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the node ids are not unique or if an edge is invalid.</p> Source code in <code>backend/src/dataclasses/charts.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Check that the nodes have unique ids and that the edges are valid.\n\n    Raises:\n        ValueError: If the node ids are not unique or if an edge is invalid.\n    \"\"\"\n    if len([node.id for node in self.nodes]) != len(set([node.id for node in self.nodes])):\n        raise ValueError('The node ids are not unique.')\n    node_ids = [node.id for node in self.nodes]\n    for edge in self.edges:\n        if edge.source not in node_ids or edge.target not in node_ids:\n            raise ValueError('The edge is invalid.')\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.charts.MultiDataSeries","title":"<code>MultiDataSeries</code>  <code>dataclass</code>","text":"<p>A data series.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the data series.</p> <code>series</code> <code>list[DataItem]</code> <p>The series of the data series.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.MultiDataSeries.from_pandas","title":"<code>from_pandas(df, name)</code>  <code>classmethod</code>","text":"<p>Create a data series from a pandas series. The index of the pandas series must have one or two levels. If the index has one level, the data series will contain one series. If the index has two levels, the data series will contain multiple series (one for each value of the first level).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Series</code> <p>The data of the data series.</p> required <code>name</code> <code>str</code> <p>The name of the data series.</p> required <p>Returns:</p> Name Type Description <code>MultiDataSeries</code> <code>MultiDataSeries</code> <p>The data series.</p> Source code in <code>backend/src/dataclasses/charts.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df: pd.DataFrame | pd.Series, name: str) -&gt; \"MultiDataSeries\":\n    \"\"\"\n    Create a data series from a pandas series. The index of the pandas series must have one or two levels. If the\n    index has one level, the data series will contain one series. If the index has two levels, the data series will\n    contain multiple series (one for each value of the first level).\n\n    Args:\n        df (pd.Series): The data of the data series.\n        name (str): The name of the data series.\n\n    Returns:\n        MultiDataSeries: The data series.\n    \"\"\"\n    series = []\n    if df.index.nlevels == 1:\n        series.append(DataSeries.from_dict(\n            name=name,\n            data=df.to_dict(),\n            sort_by=df.index.name\n        ))\n    elif df.index.nlevels == 2:\n        for index, row in df.unstack().iterrows():\n            series.append(DataSeries.from_dict(\n                name=str(index),\n                data=row.to_dict(),\n                sort_by=df.index.names[1]\n            ))\n    else:\n        raise ValueError('The index has more than two levels.')\n\n    return MultiDataSeries(\n        name=name,\n        series=series,\n    )\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.charts.Node","title":"<code>Node</code>  <code>dataclass</code>","text":"<p>A node in a graph.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The id of the node.</p> <code>label</code> <code>str</code> <p>The label of the node.</p> <code>value</code> <code>float | None</code> <p>The value of the node.</p>"},{"location":"dataclasses/#backend.src.dataclasses.charts.Variant","title":"<code>Variant</code>  <code>dataclass</code>","text":"<p>A variant of the process.</p> <p>Attributes:</p> Name Type Description <code>activities</code> <code>list[str]</code> <p>The activities of the variant.</p> <code>count</code> <code>int</code> <p>The number of occurrences of the variant.</p> <code>frequency</code> <code>float</code> <p>The frequency of the variant.</p> <code>distribution</code> <code>dict[str, int]</code> <p>The distribution of the variant based on another attribute.</p> <code>id</code> <code>int</code> <p>The hash of the variant.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters","title":"<code>filters</code>","text":""},{"location":"dataclasses/#backend.src.dataclasses.filters.BaseFilter","title":"<code>BaseFilter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A base filter.</p> <p>Attributes:</p> Name Type Description <code>attribute_name</code> <code>str</code> <p>The name of the attribute.</p> <code>operator</code> <code>FilterOperator</code> <p>The operator of the filter.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.BaseFilter.filter_value","title":"<code>filter_value: list[str] | str | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the value to filter for.</p> <p>Returns:</p> Type Description <code>list[str] | str | float | None</code> <p>The value to filter for.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.BaseFilter.supported_operators","title":"<code>supported_operators()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Returns the supported operators for the filter.</p> <p>Returns:</p> Type Description <code>tuple[FilterOperator, ...]</code> <p>The supported operators for the filter.</p> Source code in <code>backend/src/dataclasses/filters.py</code> <pre><code>@classmethod\n@abstractmethod\ndef supported_operators(cls) -&gt; tuple[FilterOperator, ...]:\n    \"\"\"\n    Returns the supported operators for the filter.\n\n    Returns:\n        (tuple[FilterOperator, ...]): The supported operators for the filter.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.filters.CategoricalFilter","title":"<code>CategoricalFilter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>BaseFilter</code></p> <p>A categorical filter. The filter can be used to filter for categorical attributes. Currently, the filter supports the following operators:</p> <ul> <li>IS_EMPTY</li> <li>NOT_EMPTY</li> <li>EQUALS</li> <li>NOT_EQUALS</li> <li>CONTAINS</li> <li>NOT_CONTAINS</li> </ul> <p>Attributes:</p> Name Type Description <code>values</code> <code>list[str] | None</code> <p>The values to filter for.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.CategoricalFilter.filter_value","title":"<code>filter_value: list[str] | str | None</code>  <code>property</code>","text":"<p>Returns the value to filter for. If the operator does not accept a value, None is returned. If the operator accepts multiple values, the values are returned as a list, otherwise the first value is returned.</p> <p>Returns:</p> Type Description <code>list[str] | str | None</code> <p>The value to filter for.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.CategoricalFilter.supported_operators","title":"<code>supported_operators()</code>  <code>classmethod</code>","text":"<p>Returns the supported operators for the filter.</p> <p>Returns:</p> Type Description <code>tuple[FilterOperator, ...]</code> <p>The supported operators for the filter.</p> Source code in <code>backend/src/dataclasses/filters.py</code> <pre><code>@classmethod\ndef supported_operators(cls) -&gt; tuple[FilterOperator, ...]:\n    \"\"\"\n    Returns the supported operators for the filter.\n\n    Returns:\n        (tuple[FilterOperator, ...]): The supported operators for the filter.\n    \"\"\"\n    return (\n        FilterOperator.IS_EMPTY,\n        FilterOperator.IS_NOT_EMPTY,\n        FilterOperator.EQUALS,\n        FilterOperator.NOT_EQUALS,\n        FilterOperator.CONTAINS,\n        FilterOperator.NOT_CONTAINS)\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator","title":"<code>FilterOperator</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enumeration of the supported filter operators.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.CONTAINS","title":"<code>CONTAINS = 'contains'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The contains operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.EQUALS","title":"<code>EQUALS = 'equals'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The equals operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.GREATER_THAN","title":"<code>GREATER_THAN = 'greater_than'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The greater than operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.GREATER_THAN_OR_EQUALS","title":"<code>GREATER_THAN_OR_EQUALS = 'greater_than_or_equals'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The greater than or equals operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.IS_EMPTY","title":"<code>IS_EMPTY = 'is_empty'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The is empty operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.IS_NOT_EMPTY","title":"<code>IS_NOT_EMPTY = 'is_not_empty'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The is not empty operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.LESS_THAN","title":"<code>LESS_THAN = 'less_than'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The less than operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.LESS_THAN_OR_EQUALS","title":"<code>LESS_THAN_OR_EQUALS = 'less_than_or_equals'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The less than or equals operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.NOT_CONTAINS","title":"<code>NOT_CONTAINS = 'not_contains'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The not contains operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.NOT_EQUALS","title":"<code>NOT_EQUALS = 'not_equals'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The not equals operator.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.accepts_multiple_values","title":"<code>accepts_multiple_values: bool</code>  <code>property</code>","text":"<p>Returns whether the operator accepts multiple values.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the operator accepts multiple values.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.accepts_no_value","title":"<code>accepts_no_value: bool</code>  <code>property</code>","text":"<p>Returns whether the operator accepts no value.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the operator accepts no value.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.FilterOperator.accepts_single_value","title":"<code>accepts_single_value</code>  <code>property</code>","text":"<p>Returns whether the operator accepts a single value.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the operator accepts a single value.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.NumericalFilter","title":"<code>NumericalFilter</code>  <code>dataclass</code>","text":"<p>             Bases: <code>BaseFilter</code></p> <p>A numerical filter. The filter can be used to filter for numerical attributes. Currently, the filter supports the following operators:</p> <ul> <li>IS_EMPTY</li> <li>NOT_EMPTY</li> <li>EQUALS</li> <li>NOT_EQUALS</li> <li>LESS_THAN</li> <li>LESS_THAN_OR_EQUALS</li> <li>GREATER_THAN</li> <li>GREATER_THAN_OR_EQUALS</li> </ul> <p>Attributes:</p> Name Type Description <code>value</code> <code>float | None</code> <p>The value to filter for.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.NumericalFilter.filter_value","title":"<code>filter_value: float | None</code>  <code>property</code>","text":"<p>Returns the value to filter for.</p> <p>Returns:</p> Type Description <code>float | None</code> <p>The value to filter for.</p>"},{"location":"dataclasses/#backend.src.dataclasses.filters.NumericalFilter.supported_operators","title":"<code>supported_operators()</code>  <code>classmethod</code>","text":"<p>Returns the supported operators for the filter.</p> <p>Returns:</p> Type Description <code>tuple[FilterOperator, ...]</code> <p>The supported operators for the filter.</p> Source code in <code>backend/src/dataclasses/filters.py</code> <pre><code>@classmethod\ndef supported_operators(cls) -&gt; tuple[FilterOperator, ...]:\n    \"\"\"\n    Returns the supported operators for the filter.\n\n    Returns:\n        (tuple[FilterOperator, ...]): The supported operators for the filter.\n    \"\"\"\n    return (\n        FilterOperator.IS_EMPTY,\n        FilterOperator.IS_NOT_EMPTY,\n        FilterOperator.EQUALS,\n        FilterOperator.NOT_EQUALS,\n        FilterOperator.LESS_THAN,\n        FilterOperator.LESS_THAN_OR_EQUALS,\n        FilterOperator.GREATER_THAN,\n        FilterOperator.GREATER_THAN_OR_EQUALS)\n</code></pre>"},{"location":"dataclasses/#backend.src.dataclasses.requests","title":"<code>requests</code>","text":""},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureGraphRequest","title":"<code>DejureGraphRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>FilteredRequest</code></p> <p>A request for a dejure graph. The graph will be calculated based on the given filters. The graph will be disaggregated by the given disaggregation attribute. The graph will show the given merics.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p> <code>statistic</code> <code>DejureStatisticType</code> <p>The metric to request.</p> <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The attribute to disaggregate the graph by.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType","title":"<code>DejureStatisticType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enumeration of the metric types in dejure graph that can be requested.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.DROP","title":"<code>DROP = 'drop'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The percentage of disaggregated patient that drop in each activity.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.MAX","title":"<code>MAX = 'max'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The max duration between two events.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.MEAN","title":"<code>MEAN = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mean duration between two events.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.MEDIAN","title":"<code>MEDIAN = 'median'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The median duration between two events.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.MIN","title":"<code>MIN = 'min'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The min duration between two events.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DejureStatisticType.REMAIN","title":"<code>REMAIN = 'remain'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The percentage of activity that goes to the next activity.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DfgRequest","title":"<code>DfgRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>FilteredRequest</code></p> <p>A request for a DFG. The graph will be calculated based on the given filters.The DFG will be disaggregated by the given disaggregation attribute.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p> <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The attribute to disaggregate the distribution by.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.DistributionRequest","title":"<code>DistributionRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>FilteredRequest</code></p> <p>A request for a distribution. The distribution will be calculated based on the given filters. The distribution will be disaggregated by the given disaggregation attribute.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p> <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The attribute to disaggregate the distribution by.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.FilteredRequest","title":"<code>FilteredRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A base request with filters.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiRequest","title":"<code>KpiRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>FilteredRequest</code></p> <p>A request for a KPI. The KPI will be calculated based on the given filters. The KPI will be disaggregated by the given disaggregation attribute. The KPI will be displayed in the legend by the given legend attribute.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p> <code>kpi</code> <code>KpiType</code> <p>The KPI to request.</p> <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The attribute to disaggregate the KPI by.</p> <code>legend_attribute</code> <code>DisaggregationAttribute | None</code> <p>The attribute to display the KPI in the legend by. Defaults to None.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType","title":"<code>KpiType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enumeration of the KPI types that can be requested.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.AUTHORIZATION_TO_PROCUREMENT","title":"<code>AUTHORIZATION_TO_PROCUREMENT = 'authorization_to_procurement'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The authorization to procurement KPI. This KPI measures the average time from authorization to procurement.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.BUREAUCRATIC_DURATION","title":"<code>BUREAUCRATIC_DURATION = 'bureaucratic_duration'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The bureaucratic duration KPI. This KPI measures the average bureaucratic duration.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.DROP_OUT","title":"<code>DROP_OUT = 'drop_out'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The drop out KPI. The KPI measures the percentage of cases that drop out.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.EVALUATION_TO_APPROACH","title":"<code>EVALUATION_TO_APPROACH = 'evaluation_to_approach'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The evaluation to approach KPI. This KPI measures the average time from evaluation to approach.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.HAPPY_PATH_ADHERENCE","title":"<code>HAPPY_PATH_ADHERENCE = 'happy_path_adherence'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The happy path adherence KPI. The KPI measures the percentage of cases that follow the happy path.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.KpiType.PERMUTED_PATH_ADHERENCE","title":"<code>PERMUTED_PATH_ADHERENCE = 'permuted_path_adherence'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The permuted path adherence KPI. The KPI measures the percentage of cases that follow a permuted path.</p>"},{"location":"dataclasses/#backend.src.dataclasses.requests.VariantListRequest","title":"<code>VariantListRequest</code>  <code>dataclass</code>","text":"<p>             Bases: <code>FilteredRequest</code></p> <p>A request for a list of variants.</p> <p>Attributes:</p> Name Type Description <code>filters</code> <code>list[BaseFilter] | None</code> <p>The filters of the request. Defaults to None.</p> <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The attribute to disaggregate the variants by.</p>"},{"location":"event_log_extraction/","title":"Event Log Extraction","text":"<p>This script is used to extract an event log from the original data. We use the Organ Retrieval and Collection of Health Information for Donation (ORCHID) dataset for this purpose. As the dataset is not publicly available, we cannot provide the data here. You can request access to the data and place the <code>opd.csv</code> file in the <code>backend/data/raw</code> folder.</p>"},{"location":"event_log_extraction/#events","title":"Events","text":"<p>The dataset contains the following events:</p> <ol> <li>Referral: The patient is referred to the OPO. This event is always present for each patient and a timestamp is available.</li> <li>Evaluation: The patient is evaluated by the OPO. This event is always present for each patient. However, no timestamp is available.</li> <li>Approach: The OPO approaches the patient. In the dataset, a flag is available that indicates if the patient was approached. If the patient was approached, a timestamp is available.</li> <li>Authorization: The patient is authorized for organ donation. In the dataset, a flag is available that indicates if the patient was authorized. If the patient was authorized, a timestamp is available.</li> <li>Procurement: The organs are procured. In the dataset, a flag is available that indicates if the organs were procured. If the organs were procured, a timestamp is available.</li> <li>Transplant: The organs are transplanted. In the dataset, a flag is available that indicates if the organs were transplanted. However, no timestamp is available.</li> </ol> <p>The events should happen in the above order. However, the dataset contains some inconsistencies. For example, there are cases where the patient was authorized before the patient was approached.</p> <p>For the events where no timestamp is available, we assume that the event happens one minute after the previous event.</p>"},{"location":"event_log_extraction/#patients","title":"Patients","text":"<p>The dataset contains the following information for each patient:</p> Original Name New Name Attribute Type Description <code>PatientID</code> <code>case:concept:name</code> Categorical A unique identifier for each patient. We use this as the case id. <code>OPO</code> <code>opo_id</code> Categorical The OPO that is responsible for the patient. <code>HospitalID</code> <code>hospital_id</code> Categorical The hospital where the patient was treated. <code>Age</code> <code>age</code> Numerical The age of the patient. <code>Gender</code> <code>gender</code> Categorical The gender of the patient. <code>Race</code> <code>race</code> Categorical The race of the patient. <code>brain_death</code> <code>brain_death</code> Categorical Indicates whether the patient experienced brain death. <code>Referral_Year</code> <code>referral_year</code> Categorical The year of patient referral. <code>Referral_DayofWeek</code> <code>referral_day_of_week</code> Categorical The day of the week of patient referral. <code>Cause_of_Death_UNOS</code> <code>cause_of_death</code> Categorical The cause of death according to UNOS (United Network for Organ Sharing). <code>Mechanism_of_Death</code> <code>mechanism_of_death</code> Categorical The mechanism of death for the patient. <code>Circumstances_of_Death</code> <code>circumstances_of_death</code> Categorical The circumstances surrounding the patient's death. <code>outcome_heart</code> <code>outcome_heart</code> Categorical Outcome for the heart organ. <code>outcome_liver</code> <code>outcome_liver</code> Categorical Outcome for the liver organ. <code>outcome_kidney_left</code> <code>outcome_kidney_left</code> Categorical Outcome for the left kidney organ. <code>outcome_kidney_right</code> <code>outcome_kidney_right</code> Categorical Outcome for the right kidney organ. <code>outcome_lung_left</code> <code>outcome_lung_left</code> Categorical Outcome for the left lung organ. <code>outcome_lung_right</code> <code>outcome_lung_right</code> Categorical Outcome for the right lung organ. <code>outcome_pancreas</code> <code>outcome_pancreas</code> Categorical Outcome for the pancreas organ."},{"location":"event_log_extraction/#backend.src.data.extract.extract","title":"<code>extract(raw)</code>","text":"<p>This function extracts an event log from the raw data.</p> Steps <ol> <li>Convert all time columns to datetime</li> <li>Only keep allowed categories for the outcome columns</li> <li>Iterate over all rows that correspond to a patient and add all events that happened to the patient to the event list</li> <li>Transform the event list to a dataframe</li> <li>Remove cases with missing timestamps</li> <li>Sort the event log by patient id and time</li> </ol> <p>Parameters:</p> Name Type Description Default <code>raw</code> <code>DataFrame</code> <p>The raw data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The extracted event log</p> Source code in <code>backend/src/data/extract.py</code> <pre><code>def extract(raw: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    This function extracts an event log from the raw data.\n\n    Steps:\n        1. Convert all time columns to datetime\n        2. Only keep allowed categories for the outcome columns\n        3. Iterate over all rows that correspond to a patient and add all events that happened to the patient to the event list\n        4. Transform the event list to a dataframe\n        5. Remove cases with missing timestamps\n        6. Sort the event log by patient id and time\n\n\n    Args:\n        raw (pd.DataFrame): The raw data\n\n    Returns:\n        (pd.DataFrame): The extracted event log\n    \"\"\"\n    # Convert all time columns to datetime\n    time_columns = ['time_referred', 'time_approached', 'time_authorized', 'time_procured']\n    for col in time_columns:\n        raw[col] = pd.to_datetime(raw[col], format='ISO8601')\n\n    # Only keep allowed outcomes\n    allowed_outcomes = ['Transplanted', 'Recovered for Research', 'Recovered for Transplant but not Transplanted']\n    include_outcomes = [c for c in PATIENT_DATA_MAPPING.keys() if c.startswith('outcome_')]\n    for col in include_outcomes:\n        raw[col] = raw[col].astype('category').cat.set_categories(allowed_outcomes)\n\n    events_list = []\n    # Iterate over all rows that correspond to a patient and add all events that happened to the patient to the event list\n    for i, row in raw.iterrows():\n        # Collect patient data by mapping the original column names to the new column names\n        patient_data = {v: row[k] for k, v in PATIENT_DATA_MAPPING.items()}\n\n        # Add the referral and evaluation events for each patient as all patients were referred and evaluated\n        events_list.append({'concept:name': 'Referral', 'time:timestamp': row['time_referred']} | patient_data)\n\n        # Unfortunately, the evaluation time is not available in the dataset\n        # We assume that the evaluation happens one minute after the referral\n        events_list.append(\n            {'concept:name': 'Evaluation',\n             'time:timestamp': row['time_referred'] + Timedelta(minutes=1)} | patient_data)\n\n        # Add each activity only if its corresponding entry is not False\n        if row['approached']:\n            events_list.append({'concept:name': 'Approach', 'time:timestamp': row['time_approached']} | patient_data)\n\n        if row['authorized']:\n            events_list.append(\n                {'concept:name': 'Authorization', 'time:timestamp': row['time_authorized']} | patient_data)\n\n        if row['procured']:\n            events_list.append({'concept:name': 'Procurement', 'time:timestamp': row['time_procured']} | patient_data)\n\n        # Unfortunately, the transplant time is not available in the dataset\n        # We assume that the transplant happens one minute after the procurement\n        if row['transplanted']:\n            events_list.append({'concept:name': 'Transplant',\n                                'time:timestamp': row['time_procured'] + Timedelta(minutes=1)} | patient_data)\n\n    # Transform the event list to a dataframe\n    columns = ['case:concept:name', 'concept:name', 'time:timestamp'] + list(PATIENT_ATTRIBUTES.keys())\n    event_log = pd.DataFrame(events_list, columns=columns)\n\n    # Get cases where a timestamp is missing\n    cases_with_missing_timestamps = event_log[event_log['time:timestamp'].isna()]['case:concept:name'].unique()\n    print(f'Found {len(cases_with_missing_timestamps)} cases with missing timestamps. Removing them ...')\n\n    # Remove cases with missing timestamps\n    event_log = event_log[~event_log['case:concept:name'].isin(cases_with_missing_timestamps)]\n\n    # Sort the event log by patient id and time\n    event_log = event_log.sort_values(by=['case:concept:name', 'time:timestamp'])\n\n    return event_log\n</code></pre>"},{"location":"process_mining/dejure/","title":"Dejure","text":""},{"location":"process_mining/dejure/#backend.src.process_mining.dejure.get_dejure_drop_graph","title":"<code>get_dejure_drop_graph(el, disaggregation_column)</code>","text":"<p>Generates a graph representing the dejure DFG with counts of end activities  and the dropout rate of each activity considering a specified disaggregation  column.</p> <p>Args:      el (pd.DataFrame): The event log.      disaggregation_column (str): Disaggregation attribute column</p> <p>Returns:</p> Name Type Description <code>Graph</code> <code>Graph</code> <p>A Graph object representing the dejure DFG with drop out information.</p> Source code in <code>backend/src/process_mining/dejure.py</code> <pre><code>def get_dejure_drop_graph(el: pd.DataFrame, disaggregation_column: str) -&gt; Graph:\n    \"\"\" Generates a graph representing the dejure DFG with counts of end activities\n        and the dropout rate of each activity considering a specified disaggregation\n        column.\n\n        Args:\n            el (pd.DataFrame): The event log.\n            disaggregation_column (str): Disaggregation attribute column\n\n       Returns:\n           Graph: A Graph object representing the dejure DFG\n            with drop out information.\n       \"\"\"\n\n    # Calculate the end activity counts considering disaggregation column\n    disaggregation_last_act = el.groupby(by=['case:concept:name']).last()[[disaggregation_column,\n                                                                           'concept:name']].value_counts()\n    # Calculate the end activity counts\n    last_act_freq = disaggregation_last_act.groupby('concept:name').sum()\n    nodes = [Node(id=value, label=value, value=count) for value, count in last_act_freq.items()]\n    nodes.append(Node(id='Referral', label='Referral', value=0))\n\n    variant = get_dejure_variant(el)\n\n    # Group the dejure cases by disaggregation_column, disregarding the disaggregation_column of nan\n    grouped_dfg = variant.groupby(disaggregation_column, observed=False).apply(lambda x: discover_dfg(x)[0])\n\n    all_edges = []\n\n    # Calculate the percentage of one activity that goes to the next activity\n    for index, relation in grouped_dfg.items():\n        edges = [\n            Edge(source=source, target=target, label=index,\n                value=(disaggregation_last_act.loc[index, source] / last_act_freq[source]) if last_act_freq.get(\n                    source, 0) != 0 else 0\n            )\n            for (source, target), freq in relation.items()\n        ]\n        all_edges.extend(edges)\n\n    return Graph(\n        name='Dejure-DFG',\n        edges=all_edges,\n        nodes=nodes,\n    )\n</code></pre>"},{"location":"process_mining/dejure/#backend.src.process_mining.dejure.get_dejure_remain_graph","title":"<code>get_dejure_remain_graph(el, disaggregation_column)</code>","text":"<p>Generates a graph representing the dejure DFG with activity frequencies and the percentage of each activity that goes to the next activity considering a specified disaggregation column.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>Disaggregation attribute column</p> required <p>Returns:</p> Name Type Description <code>Graph</code> <code>Graph</code> <p>A Graph object representing the dejure DFG</p> <code>Graph</code> <p>with activity frequencies and percentage.</p> Source code in <code>backend/src/process_mining/dejure.py</code> <pre><code>def get_dejure_remain_graph(el: pd.DataFrame, disaggregation_column: str) -&gt; Graph:\n    \"\"\"\n        Generates a graph representing the dejure DFG with activity frequencies\n        and the percentage of each activity that goes to the next activity\n        considering a specified disaggregation column.\n\n        Args:\n            el (pd.DataFrame): The event log.\n            disaggregation_column (str): Disaggregation attribute column\n\n        Returns:\n            Graph: A Graph object representing the dejure DFG\n            with activity frequencies and percentage.\n        \"\"\"\n\n    # Drop the disaggregation_column of nan and calculate the frequency of each activity\n    act_freq = el.dropna(subset=[disaggregation_column])['concept:name'].value_counts()\n    nodes = [Node(id=value, label=value, value=count) for value, count in act_freq.items()]\n\n    variant = get_dejure_variant(el)\n\n    # Group the dejure cases by disaggregation_column, disregarding the disaggregation_column of nan\n    grouped_dfg = variant.groupby(disaggregation_column, observed=False).apply(lambda x: discover_dfg(x)[0])\n\n    all_edges = []\n\n    # Calculate the percentage of one activity that goes to the next activity\n    for index, relation in grouped_dfg.items():\n        edges = [Edge(source=source, target=target, label=index, value=freq / act_freq[source])\n                 for (source, target), freq in relation.items()]\n        all_edges.extend(edges)\n\n    return Graph(\n        name='Dejure-DFG',\n        edges=all_edges,\n        nodes=nodes,\n    )\n</code></pre>"},{"location":"process_mining/dejure/#backend.src.process_mining.dejure.get_dejure_time_graph","title":"<code>get_dejure_time_graph(el, disaggregation_column, statistic)</code>","text":"<p>Generates a graph representing the dejure DFG with performance statistics considering a specified disaggregation attribute.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>Disaggregation attribute column.</p> required <code>statistic</code> <code>str</code> <p>The performance statistic to be considered.</p> required <p>Returns:</p> Name Type Description <code>Graph</code> <code>Graph</code> <p>A Graph object representing the dejure DFG</p> <code>Graph</code> <p>with performance statistics.</p> Source code in <code>backend/src/process_mining/dejure.py</code> <pre><code>def get_dejure_time_graph(el: pd.DataFrame, disaggregation_column: str, statistic: str) -&gt; Graph:\n    \"\"\"\n        Generates a graph representing the dejure DFG with performance\n        statistics considering a specified disaggregation attribute.\n\n        Args:\n            el (pd.DataFrame): The event log.\n            disaggregation_column (str): Disaggregation attribute column.\n            statistic (str): The performance statistic to be considered.\n\n        Returns:\n            Graph: A Graph object representing the dejure DFG\n            with performance statistics.\n        \"\"\"\n\n    # Drop the disaggregation_column of nan and calculate the frequency of each activity\n    act_freq = el.dropna(subset=[disaggregation_column])['concept:name'].value_counts()\n    nodes = [Node(id=value, label=value, value=count) for value, count in act_freq.items()]\n\n    variant = get_dejure_variant(el)\n\n    # Group the dejure cases by disaggregation_column, disregarding the disaggregation_column of nan\n    grouped_dfg = variant.groupby(disaggregation_column, observed=False).apply(lambda x: discover_performance_dfg(x)[0])\n\n    all_edges = []\n    for index, relation in grouped_dfg.items():\n        edges = [Edge(source=source, target=target, label=index, value=performance[statistic] / 60)\n                 for (source, target), performance in relation.items()]\n        all_edges.extend(edges)\n\n    return Graph(\n        name='Dejure-DFG',\n        edges=all_edges,\n        nodes=nodes,\n    )\n</code></pre>"},{"location":"process_mining/dejure/#backend.src.process_mining.dejure.get_dejure_variant","title":"<code>get_dejure_variant(el)</code>","text":"<p>Extracts the dejure variants from the input DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame representing all cease of the dejure variants.</p> Source code in <code>backend/src/process_mining/dejure.py</code> <pre><code>def get_dejure_variant(el: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n     Extracts the dejure variants from the input DataFrame.\n\n     Args:\n         el (pd.DataFrame): The event log.\n\n     Returns:\n         pd.DataFrame: A DataFrame representing all cease of the dejure variants.\n     \"\"\"\n    # compute the variants to filter\n    dejure_list = [DE_JURE_VARIANT[:i + 1] for i in range(len(DE_JURE_VARIANT))]\n\n    # filter only dejure variant\n    variant = filter_variants(el, dejure_list, retain=True)\n\n    return variant\n</code></pre>"},{"location":"process_mining/dfg/","title":"DFG","text":""},{"location":"process_mining/dfg/#backend.src.process_mining.dfg.get_dfg","title":"<code>get_dfg(el)</code>","text":"<p>Generate a Process Mining DFG based on the given event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>The DFG of the event log with absolute frequencies as edge values.</p> Source code in <code>backend/src/process_mining/dfg.py</code> <pre><code>def get_dfg(el: pd.DataFrame) -&gt; Graph:\n    \"\"\"\n    Generate a Process Mining DFG based on the given event log.\n\n    Args:\n        el (pd.DataFrame): The event log.\n\n    Returns:\n        (Graph): The DFG of the event log with absolute frequencies as edge values.\n    \"\"\"\n\n    # find the directly-following graph\n    dfg, start_activities, end_activities = discover_dfg(el)\n\n    # transform into graph data structure\n    edges = [Edge(source=source, target=target, label=None, value=freq) for (source, target), freq in dfg.items()]\n    node_ids = set([edge.source for edge in edges] + [edge.target for edge in edges])\n    nodes = [Node(id=activity, label=activity, value=None) for activity in node_ids]\n\n    return Graph(\n        name='DFG',\n        edges=edges,\n        nodes=nodes,\n    )\n</code></pre>"},{"location":"process_mining/distribution/","title":"Distribution","text":""},{"location":"process_mining/distribution/#backend.src.process_mining.distribution.attribute_distribution","title":"<code>attribute_distribution(el, attribute)</code>","text":"<p>Calculate the distribution of each value for a specified attribute in the event log. NaN values are represented by 'None' in the result.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log DataFrame.</p> required <code>attribute</code> <code>str</code> <p>The attribute in the event log for which the distribution is calculated.</p> required <p>Returns:</p> Name Type Description <code>DataSeries</code> <code>DataSeries</code> <p>The distribution of the attribute.</p> Source code in <code>backend/src/process_mining/distribution.py</code> <pre><code>def attribute_distribution(el: pd.DataFrame, attribute: str) -&gt; DataSeries:\n    \"\"\"\n    Calculate the distribution of each value for a specified attribute in the event log. NaN values are represented\n    by 'None' in the result.\n\n    Args:\n        el (pd.DataFrame): The event log DataFrame.\n        attribute (str): The attribute in the event log for which the distribution is calculated.\n\n    Returns:\n        DataSeries: The distribution of the attribute.\n    \"\"\"\n\n    # get first row of each case\n    first_row = el.groupby('case:concept:name').first()\n    # compute distribution considering nan\n    column_distribution = first_row[attribute].value_counts(dropna=False)\n    # rename nan in index to None\n    column_distribution.index = column_distribution.index.map(lambda x: \"None\" if pd.isna(x) else x)\n\n    return DataSeries.from_dict(\n        name=attribute,\n        data=column_distribution.to_dict(),\n        sort_by=attribute\n    )\n</code></pre>"},{"location":"process_mining/event_log/","title":"Event Log","text":""},{"location":"process_mining/event_log/#backend.src.process_mining.event_log.create_bins","title":"<code>create_bins(el, disaggregation_attribute=None)</code>","text":"<p>Create bins for the given data frame. The bins will be created based on the given disaggregation attribute. If no disaggregation attribute or a categorical disaggregation attribute is given, the data frame will not be modified. If a numerical disaggregation attribute is given, the data frame will be binned based on the bins of the disaggregation attribute. The bins will be represented by the bin labels of the disaggregation attribute.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The data frame.</p> required <code>disaggregation_attribute</code> <code>DisaggregationAttribute</code> <p>The disaggregation attribute. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data frame.</p> <code>str</code> <p>The name of the column containing the disaggregation attribute.</p> Source code in <code>backend/src/process_mining/event_log.py</code> <pre><code>def create_bins(el: pd.DataFrame, disaggregation_attribute: DisaggregationAttribute | None = None) \\\n        -&gt; tuple[pd.DataFrame, str | None]:\n    \"\"\"\n    Create bins for the given data frame. The bins will be created based on the given disaggregation attribute.\n    If no disaggregation attribute or a categorical disaggregation attribute is given, the data frame will not be\n    modified. If a numerical disaggregation attribute is given, the data frame will be binned based on the bins of the\n    disaggregation attribute. The bins will be represented by the bin labels of the disaggregation attribute.\n\n    Args:\n        el (pd.DataFrame): The data frame.\n        disaggregation_attribute (DisaggregationAttribute, optional): The disaggregation attribute. Defaults to None.\n\n    Returns:\n        (pd.DataFrame): The data frame.\n        (str): The name of the column containing the disaggregation attribute.\n    \"\"\"\n    # copy the column of the disaggregation attribute to a temporary column to avoid modifying the original data frame\n    if disaggregation_attribute is not None:\n        # copy the dataframe to avoid modifying the original data frame\n        el = el.copy()\n\n        if disaggregation_attribute.type == AttributeType.NUMERICAL:\n            # bin the numerical values\n            el[disaggregation_attribute.name] = pd.cut(el[disaggregation_attribute.name],\n                                                       bins=disaggregation_attribute.get_bins(),\n                                                       labels=disaggregation_attribute.get_bin_labels())\n\n    return el, disaggregation_attribute.name if disaggregation_attribute is not None else None\n</code></pre>"},{"location":"process_mining/event_log/#backend.src.process_mining.event_log.filter_log","title":"<code>filter_log(el, filters)</code>","text":"<p>Filter the event log based on the given filters. The filters will be applied in the order they are given. As all filters are applied to the same data frame, one can think of the filters combined with a logical AND.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>filters</code> <code>list[BaseFilter]</code> <p>The filters.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The filtered event log.</p> Source code in <code>backend/src/process_mining/event_log.py</code> <pre><code>def filter_log(el: pd.DataFrame, filters: list[BaseFilter]) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter the event log based on the given filters. The filters will be applied in the order they are given.\n    As all filters are applied to the same data frame, one can think of the filters combined with a logical AND.\n\n    Args:\n        el (pd.DataFrame): The event log.\n        filters (list[BaseFilter]): The filters.\n\n    Returns:\n        (pd.DataFrame): The filtered event log.\n    \"\"\"\n    for filter in filters:\n        match filter.operator:\n            case FilterOperator.IS_EMPTY:\n                el = el[el[filter.attribute_name].isna()]\n            case FilterOperator.IS_NOT_EMPTY:\n                el = el[el[filter.attribute_name].notna()]\n            case FilterOperator.EQUALS:\n                el = el[el[filter.attribute_name] == filter.filter_value]\n            case FilterOperator.NOT_EQUALS:\n                el = el[el[filter.attribute_name] != filter.filter_value]\n            case FilterOperator.CONTAINS:\n                el = el[el[filter.attribute_name].isin(filter.filter_value)]\n            case FilterOperator.NOT_CONTAINS:\n                el = el[~el[filter.attribute_name].isin(filter.filter_value)]\n            case FilterOperator.LESS_THAN:\n                el = el[el[filter.attribute_name] &lt; filter.filter_value]\n            case FilterOperator.LESS_THAN_OR_EQUALS:\n                el = el[el[filter.attribute_name] &lt;= filter.filter_value]\n            case FilterOperator.GREATER_THAN:\n                el = el[el[filter.attribute_name] &gt; filter.filter_value]\n            case FilterOperator.GREATER_THAN_OR_EQUALS:\n                el = el[el[filter.attribute_name] &gt;= filter.filter_value]\n            case _:\n                raise ValueError('The operator is not supported.')\n\n    return el\n</code></pre>"},{"location":"process_mining/event_log/#backend.src.process_mining.event_log.load_event_log","title":"<code>load_event_log(path)</code>","text":"<p>Load the event log from the given path. The time column will be converted to datetime and the categorical columns will be converted to categorical.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the event log.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The event log.</p> Source code in <code>backend/src/process_mining/event_log.py</code> <pre><code>def load_event_log(path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load the event log from the given path. The time column will be converted to datetime and the categorical columns\n    will be converted to categorical.\n\n    Args:\n        path (str): The path to the event log.\n\n    Returns:\n        (pd.DataFrame): The event log.\n    \"\"\"\n    # Load the event log\n    df = pd.read_csv(path, sep=',')\n\n    # Convert the time columns to datetime\n    df['time:timestamp'] = pd.to_datetime(df['time:timestamp'], format='ISO8601')\n\n    # Convert the categorical columns to categorical\n    categorical_columns = [k for k, v in PATIENT_ATTRIBUTES.items() if v == AttributeType.CATEGORICAL]\n    df[categorical_columns] = df[categorical_columns].astype('category')\n\n    # Calculate all filter attribute related columns\n    process_attributes = df.groupby('case:concept:name').agg(\n        start_activity=('concept:name', 'first'),\n        end_activity=('concept:name', 'last'),\n        case_size=('concept:name', 'count'),\n        variant=('concept:name', lambda x: ' '.join(x)),\n        case_duration=('time:timestamp', lambda x: (x.iloc[-1] - x.iloc[0]).seconds)\n    )\n\n    # Convert the categorical columns to categorical\n    categorical_columns = [k for k, v in FILTER_ATTRIBUTES.items() if v == AttributeType.CATEGORICAL]\n    process_attributes[categorical_columns] = process_attributes[categorical_columns].astype('category')\n\n    # Convert the numerical columns to int as int64 cannot be serialized to JSON\n    numerical_columns = [k for k, v in FILTER_ATTRIBUTES.items() if v == AttributeType.NUMERICAL]\n    process_attributes[numerical_columns] = process_attributes[numerical_columns].astype(np.float64)\n\n    # Merge the process attributes with the event log\n    df = df.merge(process_attributes, on='case:concept:name')\n\n    return df\n</code></pre>"},{"location":"process_mining/event_log/#backend.src.process_mining.event_log.load_filter_attributes","title":"<code>load_filter_attributes(event_log)</code>","text":"<p>Load the filter attributes from the event log. For categorical attributes, all possible values will be extracted. For numerical attributes, the minimum and maximum value will be extracted.</p> <p>Parameters:</p> Name Type Description Default <code>event_log</code> <code>DataFrame</code> <p>The event log.</p> required <p>Returns:</p> Type Description <code>list[PatientAttribute]</code> <p>The filter attributes.</p> Source code in <code>backend/src/process_mining/event_log.py</code> <pre><code>def load_filter_attributes(event_log: pd.DataFrame) -&gt; list[PatientAttribute]:\n    \"\"\"\n    Load the filter attributes from the event log. For categorical attributes, all possible values will be extracted.\n    For numerical attributes, the minimum and maximum value will be extracted.\n\n    Args:\n        event_log (pd.DataFrame): The event log.\n\n    Returns:\n        (list[PatientAttribute]): The filter attributes.\n    \"\"\"\n    attributes = []\n\n    # Iterate over all filter attributes and add them to the corresponding list\n    for name, attribute_type in FILTER_ATTRIBUTES.items():\n        if attribute_type == AttributeType.CATEGORICAL:\n            attributes.append(CategoricalAttribute(name, event_log[name].cat.categories))\n        elif attribute_type == AttributeType.NUMERICAL:\n            attributes.append(NumericalAttribute(name, event_log[name].min(), event_log[name].max()))\n\n    return attributes\n</code></pre>"},{"location":"process_mining/event_log/#backend.src.process_mining.event_log.load_patient_attributes","title":"<code>load_patient_attributes(event_log)</code>","text":"<p>Load the patient attributes from the event log. For categorical attributes, all possible values will be extracted. For numerical attributes, the minimum and maximum value will be extracted.</p> <p>Parameters:</p> Name Type Description Default <code>event_log</code> <code>DataFrame</code> <p>The event log.</p> required <p>Returns:</p> Type Description <code>list[PatientAttribute]</code> <p>The patient attributes.</p> Source code in <code>backend/src/process_mining/event_log.py</code> <pre><code>def load_patient_attributes(event_log: pd.DataFrame) -&gt; list[PatientAttribute]:\n    \"\"\"\n    Load the patient attributes from the event log. For categorical attributes, all possible values will be extracted.\n    For numerical attributes, the minimum and maximum value will be extracted.\n\n    Args:\n        event_log (pd.DataFrame): The event log.\n\n    Returns:\n        (list[PatientAttribute]): The patient attributes.\n    \"\"\"\n    attributes = []\n\n    # Iterate over all patient attributes and add them to the corresponding list\n    for name, attribute_type in PATIENT_ATTRIBUTES.items():\n        if attribute_type == AttributeType.CATEGORICAL:\n            attributes.append(CategoricalAttribute(name, event_log[name].cat.categories))\n        elif attribute_type == AttributeType.NUMERICAL:\n            attributes.append(NumericalAttribute(name, event_log[name].min(), event_log[name].max()))\n\n    return attributes\n</code></pre>"},{"location":"process_mining/kpi/","title":"KPIs","text":""},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_authorization_to_procurement","title":"<code>get_authorization_to_procurement(el, disaggregation_column, legend_column)</code>","text":"<p>Calculate authorization-to-procurement duration information based on specified disaggregation attributes in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <code>legend_column</code> <code>str</code> <p>The column used for legend.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>A data series containing the mean duration between authorization and procurement for each group.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_authorization_to_procurement(el: pd.DataFrame, disaggregation_column: str,\n                                     legend_column: str) -&gt; MultiDataSeries:\n    \"\"\"\n        Calculate authorization-to-procurement duration information based on specified disaggregation attributes in\n        the event log.\n\n        Args:\n            el (pd.DataFrame): The event log.\n            disaggregation_column (str): The column used for disaggregation.\n            legend_column (str): The column used for legend.\n\n        Returns:\n            A data series containing the mean duration between authorization and procurement for each group.\n    \"\"\"\n    group = [disaggregation_column] if legend_column is None else [legend_column, disaggregation_column]\n\n    subcase_duration = _get_duration_between_activities(el, 'Authorization', 'Procurement', group)\n\n    return MultiDataSeries.from_pandas(subcase_duration, 'Authorization to procurement')\n</code></pre>"},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_bureaucratic_duration","title":"<code>get_bureaucratic_duration(el, disaggregation_column, legend_column)</code>","text":"<p>Calculate bureaucratic duration based on specified disaggregation attributes in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <code>legend_column</code> <code>str</code> <p>The column used for legend.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>A data series containing the mean duration between referral and procurement for each group.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_bureaucratic_duration(el: pd.DataFrame, disaggregation_column: str, legend_column: str) -&gt; MultiDataSeries:\n    \"\"\"\n    Calculate bureaucratic duration based on specified disaggregation attributes in the event log.\n\n    Args:\n        el (pd.DataFrame): The event log.\n        disaggregation_column (str): The column used for disaggregation.\n        legend_column (str): The column used for legend.\n\n    Returns:\n        A data series containing the mean duration between referral and procurement for each group.\n    \"\"\"\n    group = [disaggregation_column] if legend_column is None else [legend_column, disaggregation_column]\n\n    subcase_duration = _get_duration_between_activities(el, 'Referral', 'Procurement', group)\n\n    return MultiDataSeries.from_pandas(subcase_duration, 'Bureaucratic duration')\n</code></pre>"},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_dropout","title":"<code>get_dropout(el, disaggregation_column)</code>","text":"<p>Calculate dropout information based on a specified disaggregation attribute in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>The dropout information based on a specified disaggregation attribute in the event log.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_dropout(el: pd.DataFrame, disaggregation_column: str) -&gt; MultiDataSeries:\n    \"\"\"\n    Calculate dropout information based on a specified disaggregation attribute in the event log.\n\n    Args:\n        el (pd.DataFrame): The event log.\n        disaggregation_column (str): The column used for disaggregation.\n\n    Returns:\n        The dropout information based on a specified disaggregation attribute in the event log.\n    \"\"\"\n    # We start by selecting the last activity and the corresponding disaggregation attribute for each case\n    last_activities = el.groupby(by=['case:concept:name']).last()[[disaggregation_column, 'concept:name']]\n    # We then count the number of cases for each last activity and disaggregation attribute\n    last_activities_count = last_activities.value_counts()\n\n    return MultiDataSeries.from_pandas(last_activities_count, 'Dropout rate')\n</code></pre>"},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_evaluation_to_approach","title":"<code>get_evaluation_to_approach(el, disaggregation_column, legend_column)</code>","text":"<p>Calculate evaluation-to-approach duration information based on specified disaggregation attributes in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <code>legend_column</code> <code>str</code> <p>The column used for legend.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>A data series containing the mean duration between evaluation and approach for each group.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_evaluation_to_approach(el: pd.DataFrame, disaggregation_column: str, legend_column: str) -&gt; MultiDataSeries:\n    \"\"\"\n        Calculate evaluation-to-approach duration information based on specified disaggregation attributes in the\n        event log.\n\n        Args:\n            el (pd.DataFrame): The event log.\n            disaggregation_column (str): The column used for disaggregation.\n            legend_column (str): The column used for legend.\n\n        Returns:\n            A data series containing the mean duration between evaluation and approach for each group.\n        \"\"\"\n    group = [disaggregation_column] if legend_column is None else [legend_column, disaggregation_column]\n\n    subcase_duration = _get_duration_between_activities(el, 'Evaluation', 'Approach', group)\n\n    return MultiDataSeries.from_pandas(subcase_duration, 'Evaluation to approach')\n</code></pre>"},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_happy_path_adherence","title":"<code>get_happy_path_adherence(el, disaggregation_column, legend_column)</code>","text":"<p>Calculate happy path adherence proportions for different attributes in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log DataFrame.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <code>legend_column</code> <code>str | None</code> <p>The column used for legend.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>The happy path adherence proportions for different attributes in the event log.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_happy_path_adherence(el: pd.DataFrame, disaggregation_column: str,\n                             legend_column: str | None) -&gt; MultiDataSeries:\n    \"\"\"\n    Calculate happy path adherence proportions for different attributes in the event log.\n\n    Args:\n        el (pd.DataFrame): The event log DataFrame.\n        disaggregation_column (str): The column used for disaggregation.\n        legend_column (str | None): The column used for legend.\n\n    Returns:\n        The happy path adherence proportions for different attributes in the event log.\n    \"\"\"\n    group = [disaggregation_column] if legend_column is None else [legend_column, disaggregation_column]\n\n    # count the number of cases for each group\n    legend_case = el.groupby(group) \\\n        .apply(lambda x: x['case:concept:name'].nunique()) \\\n        .fillna(0)\n\n    # filter only happy path\n    variant = filter_variants(el, [DE_JURE_VARIANT], retain=True)\n\n    # count the number of happy paths for each group and divide by the number of cases in each group\n    legend_happy_proportion = variant.groupby(group) \\\n                                  .apply(lambda x: x['case:concept:name'].nunique()) / legend_case\n    legend_happy_proportion = legend_happy_proportion.fillna(0)\n\n    return MultiDataSeries.from_pandas(legend_happy_proportion, 'Happy path adherence')\n</code></pre>"},{"location":"process_mining/kpi/#backend.src.process_mining.kpi.get_permuted_path","title":"<code>get_permuted_path(el, disaggregation_column, legend_column)</code>","text":"<p>Calculate permuted path information based on specified disaggregation attributes in the event log.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>DataFrame</code> <p>The event log.</p> required <code>disaggregation_column</code> <code>str</code> <p>The column used for disaggregation.</p> required <code>legend_column</code> <code>str</code> <p>The column used for legend.</p> required <p>Returns:</p> Type Description <code>MultiDataSeries</code> <p>The permuted path information based on specified disaggregation attributes in the event log.</p> Source code in <code>backend/src/process_mining/kpi.py</code> <pre><code>def get_permuted_path(el: pd.DataFrame, disaggregation_column: str, legend_column: str | None) -&gt; MultiDataSeries:\n    \"\"\"\n    Calculate permuted path information based on specified disaggregation attributes in the event log.\n\n    Args:\n        el (pd.DataFrame): The event log.\n        disaggregation_column (str): The column used for disaggregation.\n        legend_column (str): The column used for legend.\n\n    Returns:\n        The permuted path information based on specified disaggregation attributes in the event log.\n    \"\"\"\n    group = [disaggregation_column] if legend_column is None else [legend_column, disaggregation_column]\n\n    # filter out happy path\n    permuted_paths = filter_variants(el, [DE_JURE_VARIANT], retain=False)\n\n    # count the number of cases for each group\n    permuted_paths_count = permuted_paths.groupby(by=group) \\\n        .apply(lambda x: x['case:concept:name'].nunique()) \\\n        .fillna(0)\n\n    return MultiDataSeries.from_pandas(permuted_paths_count, 'Permuted path adherence')\n</code></pre>"},{"location":"process_mining/variants/","title":"Variants","text":""},{"location":"process_mining/variants/#backend.src.process_mining.variants.get_variants_with_case_ids","title":"<code>get_variants_with_case_ids(el)</code>","text":"<p>Returns a dictionary of variants (tuples of activity names) and the case ids that belong to them.</p> <p>Attributes:</p> Name Type Description <code>el</code> <code>DataFrame</code> <p>The event log.</p> <p>Returns:</p> Type Description <code>dict[Collection[str], list[str]]</code> <p>A dictionary of variants and the case ids that belong to them.</p> Source code in <code>backend/src/process_mining/variants.py</code> <pre><code>def get_variants_with_case_ids(el: pd.DataFrame) -&gt; dict[Collection[str], list[str]]:\n    \"\"\"\n    Returns a dictionary of variants (tuples of activity names) and the case ids that belong to them.\n\n    Attributes:\n        el (pd.DataFrame): The event log.\n\n    Returns:\n        A dictionary of variants and the case ids that belong to them.\n    \"\"\"\n    # apply the pm4py function to get the variant per case\n    _, cases = pandas_numpy_variants.apply(el, parameters={})\n\n    # switch keys and values in the dictionary to get the list of cases per variant\n    variants = {}\n    for case_id, variant in cases.items():\n        if variant not in variants:\n            variants[variant] = []\n        variants[variant].append(case_id)\n\n    return variants\n</code></pre>"},{"location":"process_mining/variants/#backend.src.process_mining.variants.get_variants_with_frequencies","title":"<code>get_variants_with_frequencies(el, disaggregation_column)</code>","text":"<p>Returns a list of variants with their frequencies and distributions of the given disaggregation attribute. The variants are sorted by their frequency in descending order.</p> <p>Attributes:</p> Name Type Description <code>el</code> <code>DataFrame</code> <p>The event log.</p> <code>disaggregation_column</code> <code>str</code> <p>The name of the column to disaggregate the variants.</p> <p>Returns:</p> Type Description <code>list[Variant]</code> <p>A list of variants with their frequencies and distributions of the given disaggregation attribute.</p> Source code in <code>backend/src/process_mining/variants.py</code> <pre><code>def get_variants_with_frequencies(el: pd.DataFrame, disaggregation_column: str) -&gt; list[Variant]:\n    \"\"\"\n    Returns a list of variants with their frequencies and distributions of the given disaggregation attribute.\n    The variants are sorted by their frequency in descending order.\n\n    Attributes:\n        el (pd.DataFrame): The event log.\n        disaggregation_column (str): The name of the column to disaggregate the variants.\n\n    Returns:\n        A list of variants with their frequencies and distributions of the given disaggregation attribute.\n    \"\"\"\n    variants = get_variants_with_case_ids(el)\n    total_case_count = el['case:concept:name'].nunique()\n\n    result: list[Variant] = []\n    for variant, case_ids in variants.items():\n        distribution = el.loc[el['case:concept:name'].isin(case_ids)] \\\n            .groupby('case:concept:name') \\\n            .first()[disaggregation_column] \\\n            .value_counts(dropna=False)\n        distribution.index = distribution.index.fillna('None')\n\n        result.append(Variant(\n            activities=list(variant),\n            count=len(case_ids),\n            frequency=len(case_ids) / total_case_count,\n            distribution=DataSeries.from_dict(\n                data=distribution.to_dict(),\n                name=disaggregation_column,\n                sort_by=disaggregation_column\n            )\n        ))\n\n    # sort the variants by their frequency\n    result.sort(key=lambda v: v.frequency, reverse=True)\n\n    return result\n</code></pre>"}]}